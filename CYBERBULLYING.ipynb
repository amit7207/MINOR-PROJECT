{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9909be5d"
      },
      "source": [
        "**IMPORT DATASET**:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7770636",
        "outputId": "d078e593-8ef0-4c09-cab1-c38c3752bd5f"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/DATASET/cyberbully_data.csv')\n",
        "print(df.head())\n",
        "print(df.info())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
            "0           0      3            0                   0        3      2   \n",
            "1           1      3            0                   3        0      1   \n",
            "2           2      3            0                   3        0      1   \n",
            "3           3      3            0                   2        1      1   \n",
            "4           4      6            0                   6        0      1   \n",
            "\n",
            "                                               tweet  \n",
            "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
            "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
            "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
            "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
            "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 24783 entries, 0 to 24782\n",
            "Data columns (total 7 columns):\n",
            " #   Column              Non-Null Count  Dtype \n",
            "---  ------              --------------  ----- \n",
            " 0   Unnamed: 0          24783 non-null  int64 \n",
            " 1   count               24783 non-null  int64 \n",
            " 2   hate_speech         24783 non-null  int64 \n",
            " 3   offensive_language  24783 non-null  int64 \n",
            " 4   neither             24783 non-null  int64 \n",
            " 5   class               24783 non-null  int64 \n",
            " 6   tweet               24783 non-null  object\n",
            "dtypes: int64(6), object(1)\n",
            "memory usage: 1.3+ MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa2d47a6"
      },
      "source": [
        "**DATA PREPROCESSING**:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "752e199d",
        "outputId": "66c1f09c-1807-45b5-fd79-fda114e8c137"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "print(\"NLTK stopwords and wordnet data downloaded and libraries imported.\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK stopwords and wordnet data downloaded and libraries imported.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd4c461f"
      },
      "source": [
        "**CONVERT TEXT TO LOWER CASE**:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b70db0d4",
        "outputId": "c76a4c96-5537-46cb-a3ff-8dafc03ed8a5"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower() # Convert to lowercase\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text) # Remove special characters and numbers\n",
        "    words = text.split() # Tokenize\n",
        "    words = [word for word in words if word not in stop_words] # Remove stopwords\n",
        "    words = [lemmatizer.lemmatize(word) for word in words] # Lemmatize\n",
        "    return ' '.join(words)\n",
        "\n",
        "df['cleaned_tweet'] = df['tweet'].apply(preprocess_text)\n",
        "\n",
        "print(df[['tweet', 'cleaned_tweet']].head())\n",
        "print(\"Text preprocessing complete and 'cleaned_tweet' column added.\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               tweet  \\\n",
            "0  !!! RT @mayasolovely: As a woman you shouldn't...   \n",
            "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...   \n",
            "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...   \n",
            "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...   \n",
            "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...   \n",
            "\n",
            "                                       cleaned_tweet  \n",
            "0  rt mayasolovely woman shouldnt complain cleani...  \n",
            "1  rt mleew boy dat coldtyga dwn bad cuffin dat h...  \n",
            "2  rt urkindofbrand dawg rt sbabylife ever fuck b...  \n",
            "3           rt cganderson vivabased look like tranny  \n",
            "4  rt shenikaroberts shit hear might true might f...  \n",
            "Text preprocessing complete and 'cleaned_tweet' column added.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "459404a2"
      },
      "source": [
        "**TARGET LABELS FOR MODEL TRAINING**:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea27a7a6",
        "outputId": "f52db921-ff27-442c-fa34-f1f1b59c8d55"
      },
      "source": [
        "print(df['class'].value_counts())\n",
        "print(df['class'].dtype)\n",
        "\n",
        "print(\"Target labels in 'class' column are already numerical and ready for model training.\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class\n",
            "1    19190\n",
            "2     4163\n",
            "0     1430\n",
            "Name: count, dtype: int64\n",
            "int64\n",
            "Target labels in 'class' column are already numerical and ready for model training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "410c59a7"
      },
      "source": [
        "## Feature Engineering\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8900ada8"
      },
      "source": [
        "**TRAINING AND TESTING**:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7ed73c9",
        "outputId": "07f4892a-513f-4463-c784-ff19cf2c65ad"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Initialize TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000) # Limiting features to 5000 to manage dimensionality\n",
        "\n",
        "# Fit and transform the 'cleaned_tweet' column\n",
        "X = tfidf_vectorizer.fit_transform(df['cleaned_tweet'])\n",
        "\n",
        "# Define target variable 'y'\n",
        "y = df['class']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"TF-IDF vectorization complete. Data split into training and testing sets.\")\n",
        "print(f\"Shape of X_train: {X_train.shape}\")\n",
        "print(f\"Shape of X_test: {X_test.shape}\")\n",
        "print(f\"Shape of y_train: {y_train.shape}\")\n",
        "print(f\"Shape of y_test: {y_test.shape}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF vectorization complete. Data split into training and testing sets.\n",
            "Shape of X_train: (19826, 5000)\n",
            "Shape of X_test: (4957, 5000)\n",
            "Shape of y_train: (19826,)\n",
            "Shape of y_test: (4957,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8ac516a"
      },
      "source": [
        "## Train and Evaluate Model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad61fadd"
      },
      "source": [
        "**Using Logistic Regression **:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa47e2a6",
        "outputId": "12104d8f-3ac3-45c4-e6e7-4abf036ff75e"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Instantiate Logistic Regression model\n",
        "log_reg_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "log_reg_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = log_reg_model.predict(X_test)\n",
        "\n",
        "# Calculate and print evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n",
        "\n",
        "# Print detailed classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Logistic Regression model trained and evaluated.\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8959\n",
            "Precision: 0.8824\n",
            "Recall: 0.8959\n",
            "F1-score: 0.8829\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.19      0.28       290\n",
            "           1       0.91      0.96      0.94      3832\n",
            "           2       0.84      0.83      0.84       835\n",
            "\n",
            "    accuracy                           0.90      4957\n",
            "   macro avg       0.78      0.66      0.69      4957\n",
            "weighted avg       0.88      0.90      0.88      4957\n",
            "\n",
            "Logistic Regression model trained and evaluated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "539f73dc",
        "outputId": "1a548ff4-51e5-469b-a2d0-d3f68a531c25"
      },
      "source": [
        "import pickle\n",
        "\n",
        "# Save the trained Logistic Regression model\n",
        "with open('logistic_regression_model.pkl', 'wb') as file:\n",
        "    pickle.dump(log_reg_model, file)\n",
        "print(\"Logistic Regression model saved as 'logistic_regression_model.pkl'\")\n",
        "\n",
        "# Save the fitted TF-IDF vectorizer\n",
        "with open('tfidf_vectorizer.pkl', 'wb') as file:\n",
        "    pickle.dump(tfidf_vectorizer, file)\n",
        "print(\"TF-IDF vectorizer saved as 'tfidf_vectorizer.pkl'\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression model saved as 'logistic_regression_model.pkl'\n",
            "TF-IDF vectorizer saved as 'tfidf_vectorizer.pkl'\n"
          ]
        }
      ]
    }
  ]
}